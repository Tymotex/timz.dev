---
title: Character Encoding Systems
description: Character encoding standards like ASCII assign a number to uniquely identify a single character.
published: false
date: 2022-06-25
thumbnail: /images/thumbnails/computer-science.png
tags:
    - Computer Science
---

import { Image } from 'src/blog-components/Image';

A character encoding standard like ASCII assigns a number to uniquely identify a single character. We call this number the code point of the character. The purpose of character encoding standard like ASCII is to store text digitally, because computers only ever work with 0s and 1s.

If we wanted to store and work with text containing these characters on a computer system, we‚Äôd need to tell the computer system what numbers correspond to what characters. In other words, we need to tell the computer what character encoding scheme to use. We could just naively go through the characters we want to encode like this:

* a will be represented by 1
* b will be represented by 2
* ...
* z will be represented by 26
* ... and so on.

This works fine, however we live in a world where computers constantly talk to each other, exchanging data over the internet or otherwise. What happens when you send a text file to your friend who has a totally different character encoding scheme to you? For information interchange like this to work, you and your friend and everyone else in the world need to agree on a standard character encoding scheme. 

## ASCII ‚Äî American Standard Code for Information Interchange

With ASCII, we‚Äôre basically assuming that all the text we ever wanted to work with *only consisted* of alphabetical characters (lowercase a-z and uppercase A-Z), numerical characters (0-9), punctuation characters, and other characters such as $, #, %, ^ and so on. On top of these human-readable characters however, we also have what we call *[control characters](https://en.wikipedia.org/wiki/Control_character)* or *non-printing character* which don‚Äôt really have written representations, but which signal important information to the computer about how text should be displayed. For example, when you hit the backspace key, that‚Äôs the same as typing the [backspace character](https://en.wikipedia.org/wiki/Backspace) (which has a code point of 8 in ASCII).

This is actually a reasonably small set, and to give each of these characters a unique code we need somewhere in the ballpark of 100 numbers. With 7 bits, we can make numbers 0-127 (since 2‚Å∑ is 128), which lets us assign a unique code point to 128 different characters. This is what the creators of ASCII decided was reasonable, and these are the character encodings that they came up with:

<Image src="/images/blogs/concepts/Character-Encoding/ASCII-table.png" alt="ASCII table" />

Often, more useful ASCII tables like [this](https://www.rapidtables.com/code/text/ascii-table.html) will provide hexadecimal values alongside the decimal values since we see hexadecimal values way more often when looking at file contents. Also, if you‚Äôre on Linux, typing `man ascii` in your terminal gives you the ASCII table ‚Äî really handy for quick lookups in CTFs.

## Unicode

There is a tradeoff with the number of characters you can encode and the number of bits you need to dedicate to representing all those characters. As English speakers, we could get away with ASCII since our alphabet is quite small. However, we need to keep in mind that computers exist everywhere at this point and the world‚Äôs language distribution looks like this:

<Image src="/images/blogs/concepts/Character-Encoding/language-distribution.png" alt="World language distribution diagram" />

How would you represent the tens of thousands of characters in the Chinese language alone? 

The *Unicode Consortium* was founded and introduced the first version of the *Unicode* standard in 1991 to solve these problems. Its character encoding system uses *up to* 32 bits to currently cover 100000+ characters, well over enough to uniquely represent ever major language‚Äôs alphabet and characters and have a bunch of leftover code points we could use to represent emojis and symbols like ‚öë or üíî.

If we use 32 bits to encode each regular English character like A or B, wouldn‚Äôt that be incredibly wasteful? If we adopt such a standard, all our text files would be 4 times larger, right? The genius of Unicode is that the characters will take as many bits as they need, we‚Äôll see this later.

Firstly, a few things to note:

* Unicode itself is split into encoding schemes UTF-8, UTF-16 and UTF-32. UTF-8 is the most prevalent character encoding you‚Äôll see on the web.
    - Note: UTF stands for ‚ÄòUnicode Transformation Format‚Äô
* We often communicate the code point of characters in the shorter format: `U+<hexademical number>`. So for example, the character ‚öë happens to have the code point `0x2691` (which in decimal is the number `9873`), so we express the Unicode code point as `U+2691`.

## UTF-8

Remember, Unicode only takes up as many bits as needed for the code point. With UTF-8, all your ASCII characters have the same code points, meaning they‚Äôll all be represented and stored in 8 bits rather than 32. So there‚Äôs absolutely no issue with adopting UTF-8 if you‚Äôre only using ASCII.

So what happens in UTF-8 if we want to represent a Chinese character like Áà± (love) which would not fit in 8 bits? We‚Äôd have to use more bytes.

The code point for Áà± happens to be `U+7231` which in binary is 01110010 00110001. So that means we‚Äôd store Áà± with 2 bytes, right? 

Unfortunately, if we directly store in the 2 bytes, 01110010 00110001, the computer would have no way to tell if it should interpret these 2 bytes as 2 separate characters or 1 character. In other words, if the computer saw 01110010 00110001, then it has no way to decide whether to show ‚ÄòÁà±‚Äô or ‚Äòr1‚Äô!


<Image src="/images/blogs/concepts/Character-Encoding/code-point-mapping-1.png" alt="Code point mapping example" width="60%" />

<Image src="/images/blogs/concepts/Character-Encoding/code-point-mapping-2.png" alt="Code point mapping example" width="60%" />

Unicode is incredibly clever in its solution to this problem: we use the first byte of the character to tell the computer *how many more bytes to expect for the current character encoding*.

* If the computer sees `0xxxxxxx`, then it knows that the current encoding uses 1 byte.
    - We have 7 free slots to store the Unicode code point.
* If the computer sees `110xxxxx`, then it knows that the current encoding uses 2 bytes.
It expects the next byte to be `10xxxxxx` .
    - The first byte gives us 5 slots and the second byte gives us 6 slots, meaning we can store code points that are representable in 11 bits, so below the value 2¬π¬π.
* If the computer sees `1110xxxx`, then it knows that the current encoding uses 3 bytes.
It expects the next 2 bytes to be `10xxxxxx` .
    - The 3 bytes give 4 + 6 + 6 = 16 bits to represent the code point.
* If the computer sees `11110xxx`, then it knows that the current encoding uses 4 bytes.
It expects the next 3 bytes to be `10xxxxxx` .
    - The 4 bytes give 3 + 6 + 6 + 6 = 21 bits to represent the code point.

This is all summarised in the following table:

<Image src="/images/blogs/concepts/Character-Encoding/multi-byte-mapping.png" alt="Multi-byte code point mapping example" width="80%" />

So, how do we represent Áà±? It looks like we‚Äôll need 3 bytes to give us enough ‚Äòfree slots‚Äô to write in the full code point for Áà±:

<Image src="/images/blogs/concepts/Character-Encoding/code-point-mapping-3.png" alt="Code point mapping example" width="60%" />

You can check what the binary looks like for different Unicode characters here: [onlineunicodetools](https://onlineunicodetools.com/convert-unicode-to-binary).

## UTF-16 and UTF-32

What‚Äôs the difference between UTF-8, UTF-16 and UTF-32? They‚Äôre all responsible for encoding Unicode characters using a multi-byte encoding like in the example shown above.

UTF-16 uses 16 bits, or 2 bytes, as the minimum number of bits to encode a single character. This means that all ASCII characters would take up 2 bytes in UTF-16, meaning it‚Äôs on average a lot more wasteful of memory and storage space than if you were to use UTF-8. Why would you ever choose UTF-16 then? It turns out that when you‚Äôre using UTF-8, many characters will extend beyond 1 byte which makes indexing into a position in a string or calculating the length of strings becomes a lot less efficient! With UTF-16, you can expect the vast majority of characters to fit within those 2 bytes, so you‚Äôd rarely have to ever extend by 1 or 2 bytes. There are also more tradeoffs beyond this.

Interestingly, UTF-16 has some associated security vulnerabilities, for example, [CVE-2008-2938](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2008-2938) and [CVE-2012-2135](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2012-2135). This has been one huge driver behind UTF-8 being the standard character encoding system for the web.

What about UTF-32? No one uses it unfortunately. It‚Äôs simply far too wasteful to use 4 bytes for everything.
